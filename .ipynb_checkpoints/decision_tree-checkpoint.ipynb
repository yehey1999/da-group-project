{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aparments and Houses Price Prediction\n",
    "### Authors: \n",
    "Cagampang, Joseph Donee Y.<br>\n",
    "Gucio, Maria Angelica<br>\n",
    "Mondejar, Yanni Jan<br>\n",
    "Rosalijos, Joshua<br>\n",
    "Verdida, Kenneth Mae<br>\n",
    "\n",
    "#### Date: March 18, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats import diagnostic as diag\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'property24.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-611fedde6495>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'property24.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# display the first five rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'property24.csv'"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "dataset_df = pd.read_csv('property24.csv')\n",
    "\n",
    "# display the first five rows\n",
    "display(dataset_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total rows with null value\n",
    "num_null =  dataset_df.isna().sum().sum()\n",
    "\n",
    "# before removing null values\n",
    "print('Before:')\n",
    "print('null values: {0}'.format(num_null))\n",
    "print('total instances: ', len(dataset_df))\n",
    "\n",
    "# display what columns has null values\n",
    "display(dataset_df.isnull().any())\n",
    "\n",
    "# drop nulls\n",
    "dataset_df = dataset_df.dropna()\n",
    "\n",
    "# redisplay\n",
    "display('-'*100)\n",
    "display(dataset_df.isnull().any())\n",
    "\n",
    "# after removing\n",
    "num_null =  dataset_df.isna().sum().sum()\n",
    "\n",
    "print('After:')\n",
    "print('null values: {0}'.format(num_null))\n",
    "print('total instances: ', len(dataset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop not needed column/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.drop(['Link'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify unique values of the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_values = dataset_df['Type'].unique()\n",
    "location_values = dataset_df['Location'].unique()\n",
    "garden_values = dataset_df['Garden'].unique()\n",
    "pet_friendly_values = dataset_df['Pet Friendly'].unique()\n",
    "\n",
    "print('Categorical Unique Values')\n",
    "\n",
    "print('-'*100)\n",
    "print('Location unique values: ')\n",
    "print(location_values)\n",
    "\n",
    "print('-'*100)\n",
    "print('Type unique values: ')\n",
    "print(type_values)\n",
    "\n",
    "print('-'*100)\n",
    "print('Garden: ')\n",
    "print(garden_values)\n",
    "\n",
    "print('-'*100)\n",
    "print('Pet Friendly: ')\n",
    "print(pet_friendly_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed duplicates\n",
    "num_duplicates = len(dataset_df) - len(dataset_df.drop_duplicates(keep=False))\n",
    "\n",
    "# before removing duplicates\n",
    "print('Before:')\n",
    "print('duplicates: {0}'.format(num_duplicates))\n",
    "print('total instances: ', len(dataset_df))\n",
    "\n",
    "# removed duplicates\n",
    "dataset_df = dataset_df.drop_duplicates()\n",
    "\n",
    "# afer removing\n",
    "print('-'*100)\n",
    "print('After:')\n",
    "print('total instances: ', len(dataset_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical variables\n",
    "dummy_type = pd.get_dummies(dataset_df['Type'], prefix='Type')\n",
    "dummy_location = pd.get_dummies(dataset_df['Location'], prefix='Location')\n",
    "dummy_pet_friendly = pd.get_dummies(dataset_df['Pet Friendly'], prefix='Pet Friendly')\n",
    "dummy_garden = pd.get_dummies(dataset_df['Garden'], prefix='Garden')\n",
    "\n",
    "\n",
    "cleaned_df = dataset_df.drop(['Location', 'Type', 'Pet Friendly', 'Garden'], axis=1)\n",
    "\n",
    "# only include n-1 for the created columns for the categorical variable\n",
    "# to avoid dummy variable trap\n",
    "\n",
    "cleaned_df['Type_Townhouse'] = dummy_type['Type_Townhouse']\n",
    "\n",
    "cleaned_df['Location_Quezon City'] = dummy_location['Location_Quezon City']\n",
    "cleaned_df['Location_Pasig City'] = dummy_location['Location_Pasig City']\n",
    "cleaned_df['Location_Muntinlupa City'] = dummy_location['Location_Muntinlupa City']\n",
    "cleaned_df['Location_Marikina City'] = dummy_location['Location_Marikina City']\n",
    "cleaned_df['Location_Paranaque City'] = dummy_location['Location_Paranaque City']\n",
    "cleaned_df['Location_Caloocan City'] = dummy_location['Location_Caloocan City']\n",
    "cleaned_df['Location_Taguig City'] = dummy_location['Location_Taguig City']\n",
    "cleaned_df['Location_Manila City'] = dummy_location['Location_Manila City']\n",
    "\n",
    "cleaned_df['Type_Townhouse'] = dummy_type['Type_Townhouse']\n",
    "\n",
    "cleaned_df['Garden_yes'] = dummy_garden['Garden_yes']\n",
    "cleaned_df['Pet_Friendly_yes'] = dummy_pet_friendly['Pet Friendly_yes']\n",
    "\n",
    "\n",
    "# Location\n",
    "#   1. Las Pinas City\n",
    "#   2. Quezon City\n",
    "#   3. Pasig City\n",
    "#   4. Muntinlupa City\n",
    "#   5. Marikina City\n",
    "#   6. Paranaque City\n",
    "#   7. Caloocan City\n",
    "#   8. Taguig City\n",
    "#   9. Manila City\n",
    "\n",
    "# Type\n",
    "#   1. House and Lot\n",
    "#   2. Townhouse\n",
    "\n",
    "# Garden\n",
    "#   1. Yes\n",
    "#   2. No\n",
    "\n",
    "# Pet Friendly\n",
    "#   1. Yes\n",
    "#   2. No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr = cleaned_df.corr()\n",
    "\n",
    "# display the correlation matrix\n",
    "display(corr)\n",
    "\n",
    "# plot the correlation heatmap\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price is drop because it is dependent variable\n",
    "cleaned_df_check = cleaned_df.drop(columns=['Price',])\n",
    "\n",
    "# the VFI does expect a constant term in the data, so we need to add one using the add_constant method\n",
    "X1 = sm.tools.add_constant(cleaned_df_check)\n",
    "\n",
    "# create the series for before the drop\n",
    "series = pd.Series([variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])], index=X1.columns)\n",
    "\n",
    "# display the series\n",
    "print('VIF score')\n",
    "print('-'*100)\n",
    "display(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before removing outliers\n",
    "print('Before:')\n",
    "print('total instances: {0}'.format(len(cleaned_df)))\n",
    "\n",
    "# filter the data frame to remove the values exceeding 3 standard deviations\n",
    "cleaned_remove_df = cleaned_df[(np.abs(stats.zscore(cleaned_df)) < 3).all(axis=1)]\n",
    "\n",
    "# what rows were removed\n",
    "cleaned_outliers_df = cleaned_df.index.difference(cleaned_remove_df.index)\n",
    "\n",
    "# total outliers\n",
    "total_outliers = len(cleaned_outliers_df)\n",
    "\n",
    "# assign the cleaned data (without outliers)\n",
    "cleaned_df = cleaned_remove_df\n",
    "\n",
    "# after removing outliers\n",
    "print('-'*100)\n",
    "print('After:')\n",
    "print('outliers: ', cleaned_outliers_df.values)\n",
    "print('total outliers index: ', len(cleaned_outliers_df))\n",
    "print('cleaned instances: ', len(cleaned_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaned_df \n",
    "    \n",
    "1. Check Null Values\n",
    "2. Drop not needed column/s\n",
    "3. Identify unique values of the categorical columns\n",
    "4. Check for duplicates\n",
    "5. Variable Creation\n",
    "6. Check Multicollinearity\n",
    "7. Check Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function importing Dataset \n",
    "def importdata(): \n",
    "    proerty = cleaned_df\n",
    "    # Printing the dataswet shape \n",
    "    print (\"Dataset Length: \", len(proerty)) \n",
    "    print (\"Dataset Shape: \", proerty.shape) \n",
    "    #balance_data = balance_data.apply(LabelEncoder().fit_transform)\n",
    "    #balance_data = balance_data.astype(int)\n",
    "    # Printing the dataset obseravtions \n",
    "    #print (\"Dataset: \",balance_data.head()) \n",
    "    return proerty \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Function to split the dataset \n",
    "def splitdataset(proerty): \n",
    "  \n",
    "    # Separating the target variable \n",
    "    X = proerty.values[:, 1:5] \n",
    "    Y = proerty.values[:, 0] \n",
    "  \n",
    "    # Splitting the dataset into train and test \n",
    "    X_train, X_test, y_train, y_test = train_test_split(  \n",
    "    X, Y, test_size = 0.3, random_state = 100) \n",
    "      \n",
    "    return X, Y, X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training with giniIndex. \n",
    "def train_using_gini(X_train, X_test, y_train): \n",
    "  \n",
    "    # Creating the classifier object \n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n",
    "            random_state = 100,max_depth=3, min_samples_leaf=5) \n",
    "  \n",
    "    # Performing training \n",
    "    clf_gini.fit(X_train, y_train) \n",
    "    return clf_gini \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training with giniIndex. \n",
    "def test_using_gini(X_test,X_train, y_test): \n",
    "  \n",
    "    # Creating the classifier object \n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n",
    "            random_state = 100,max_depth=3, min_samples_leaf=5) \n",
    "  \n",
    "    # Performing training \n",
    "    clf_gini.fit(X_test, y_test) \n",
    "    return clf_gini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# Function to perform training with entropy. \n",
    "def tarin_using_entropy(X_train, X_test, y_train): \n",
    "  \n",
    "    # Decision tree with entropy \n",
    "    clf_entropy = DecisionTreeClassifier( \n",
    "            criterion = \"entropy\", random_state = 100, \n",
    "            max_depth = 3, min_samples_leaf = 5) \n",
    "  \n",
    "    # Performing training \n",
    "    clf_entropy.fit(X_train, y_train) \n",
    "    return clf_entropy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# Function to perform training with entropy. \n",
    "def test_using_entropy(X_test,X_train, y_test): \n",
    "  \n",
    "    # Decision tree with entropy \n",
    "    clf_entropy = DecisionTreeClassifier( \n",
    "            criterion = \"entropy\", random_state = 100, \n",
    "            max_depth = 3, min_samples_leaf = 5) \n",
    "  \n",
    "    # Performing training \n",
    "    clf_entropy.fit(X_test, y_test) \n",
    "    return clf_entropy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions \n",
    "def test_prediction(X_test, clf_object): \n",
    "  \n",
    "    # Predicton on test with giniIndex \n",
    "    y_pred = clf_object.predict(X_test) \n",
    "    print(\"Test predicted values:\") \n",
    "    print(y_pred) \n",
    "    return y_pred \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions \n",
    "def train_prediction(X_train, clf_object): \n",
    "  \n",
    "    # Predicton on test with giniIndex \n",
    "    y_pred = clf_object.predict(X_train) \n",
    "    print(\"Train Predicted values:\") \n",
    "    print(y_pred) \n",
    "    return y_pred \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# Function to calculate accuracy \n",
    "def cal_accuracy(y_test,y_train, y_pred, train_pred_gini): \n",
    "      \n",
    "    print(\"Test Confusion Matrix: \")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Test accuracy : \", \n",
    "    accuracy_score(y_test,y_pred)*100) \n",
    "    \n",
    "    print (\"Train accuracy : \", \n",
    "    accuracy_score(y_train,train_pred_gini)*100) \n",
    "      \n",
    "    print(\"Report : \", \n",
    "    classification_report(y_test, y_pred)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Driver code \n",
    "def main(): \n",
    "      \n",
    "    # Building Phase \n",
    "    data = importdata() \n",
    "    X, Y, X_train, X_test, y_train, y_test = splitdataset(data) \n",
    "    \n",
    "    train_clf_gini = train_using_gini(X_train, X_test, y_train) \n",
    "    test_clf_gini = test_using_gini(X_test,X_train, y_test)  \n",
    "    \n",
    "    train_clf_entropy = tarin_using_entropy(X_train, X_test, y_train) \n",
    "    test_clf_entropy = test_using_entropy(X_test,X_train, y_test) \n",
    "    \n",
    "      \n",
    "    # Operational Phase \n",
    "    \n",
    "    print(\"\\nResults Using Gini Index:\") \n",
    "      \n",
    "    # Prediction using gini \n",
    "    test_pred_gini = test_prediction(X_test,test_clf_gini) \n",
    "    train_pred_gini = train_prediction(X_train,train_clf_gini) \n",
    "    cal_accuracy(y_test,y_train, test_pred_gini, train_pred_gini)\n",
    "    \n",
    "    \n",
    "    print(\"\\nResults Using Entropy:\") \n",
    "    #Prediction using entropy \n",
    "    test_pred_entropy = test_prediction(X_test, test_clf_entropy) \n",
    "    train_pred_entropy = train_prediction(X_train, train_clf_entropy) \n",
    "    \n",
    "    cal_accuracy(y_test, y_train,test_pred_entropy, train_pred_entropy) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling main function \n",
    "if __name__==\"__main__\": \n",
    "    main() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = ['Bedrooms', 'Bathrooms', 'Floor', 'Floors Area', 'Lot Area', 'Garage',\n",
    "                 'Reservation Fee', 'Type_Townhouse', 'Location_Quezon City',\n",
    "                 'Location_Pasig City', 'Location_Muntinlupa City',\n",
    "                 'Location_Marikina City', 'Location_Paranaque City',\n",
    "                 'Location_Caloocan City', 'Location_Taguig City',\n",
    "                 'Location_Manila City', 'Garden_yes', 'Pet_Friendly_yes']\n",
    "#esktop/2nd semester/sample/Transformed Data Set.csv\n",
    "#iris = datasets.load_iris()\n",
    "X = cleaned_df.drop('Price', axis = 1)\n",
    "y = cleaned_df['Price']\n",
    "#X = pd.DataFrame(iris.data, columns = FEATURE_NAMES)\n",
    "#y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(model, 'tree.dot', feature_names = FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_call\n",
    "check_call(['dot','-Tpng','tree.dot','-o','tree.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "img = cv2.imread('tree.png')\n",
    "plt.figure(figsize = (20, 20))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
